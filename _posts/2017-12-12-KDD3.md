---
layout: post
title: KDD algebra 2005-2006 metric 
---

For an imbalanced dataset like this one (77% positive target values), prediction accuracy functions as a [black box](https://en.wikipedia.org/wiki/Accuracy_paradox). As a commonly-used example, consider identifying cancer patients in a population where 99% of patients do not have cancer. A model that simply gave a negative prediction in every case would be "99% accurate" and totally useless in actually diagnosing cancer. 

Sometimes the minority class is obviously of higher inherent interest to me (customers who might buy my product, not pay back loans, etc.), and I would rather have false positives when the cost of a false negative is higher. Additionally, I should not be building a model to do something I can do myself. It should be providing me information I wouldn't otherwise have.

Metrics like precision and recall open the accuracy black box and tell us how well we're doing in labeling observations by class. Precision, also called positive (or negative) predictive value in medical and other diagnostic contexts, can be articulated as “If the model says a value is 1 (or 0), what is the probability it is actually 1 (or 0)?” Recall, also called sensitivity, can be expressed as “If a value is 1 (or 0), what is the probability the model correctly identifies it as 1 (or 0)?” 

(F1 score, a metric I would actually report, is the harmonic mean of precision and recall. In practice I would also report a metric like informedness (Youden's index), which takes true negatives into account, in conjuction with something like AUC/ROC.) 

For this dataset I find I don't have to make any tradeoffs; classifiers that get higher F1 majority scores also consistently get higher minority scores. If we're actually paying attention to what's happening in the data, we have a good sense of both when students will get an answer correct and when they won't. 



