---
layout: post
title: KDD algebra 2005-2006 data 
---

The 2005-2006 algebra training set, for context, contains 112 unique knowledge component strings. Transactions containing a single knowledge component make up over 50% of the dataset. (Because 80% of these transactions have positive target values (Correct First Attempt = 1) a fully na√Øve model that labeled every target value as positive would be guaranteed to misclassify 20% of these transactions, or 10% of the entire dataset.) 

There is also, from the other direction, a clear relationship between the number of knowledge components in a transaction and the likelihood of a correct answer, from about 80% for transactions with zero, one, or two components down to about 40% for transactions with five or more components. 

The number of knowledge components therefore represents a conceptual axis (in practice known as a "feature") that can reasonably be called something like "problem complexity." This is mostly worth getting into when you are formulating a different set of experimental questions and not necessarily when you just want to maximize the performance of this particular classifier, since there are many established ways to do that here with model refinement and parameter tuning without doing any of this kind of feature engineering. It's certainly not necessary to label your axes in this way for the benefit of any machine learning model, the simplest of which is perfectly capable of making a connection this straightforward on its own. (Engineering simple features like this can, however, be helpful when you want to do something like strengthen a signal by binning noisy values.) 

"Problem complexity," or at least this one aspect of it, is my first extraction, the second relating to the number of times a student has encountered the same category of problem, which might be called something like "exposure level." (I don't want to tie this to skill level because student improvement tends to peak at about 40-50 attempts at a knowledge component, after which it reverts to where it started. So they're only getting better up to a point.)
